{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "3c5c1e11-1b2b-45a1-bb69-e51c229b695f",
      "cell_type": "code",
      "source": "%pip install seaborn\n%pip install plotly\n%pip install nbformat",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1d309229-27fd-4e66-8049-2f5cc0f3a0d4",
      "cell_type": "code",
      "source": "import numpy as np\n\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model as lm\nimport plotly.express as px\nimport nbformat\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport zipfile\nimport os\n\nimport hashlib\n\n# Plot settings\nplt.rcParams['figure.figsize'] = (12, 9)\nplt.rcParams['font.size'] = 12",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f5af1bd4-04bd-4b38-bce7-b8d1fe79d0c8",
      "cell_type": "code",
      "source": "initial_data = pd.read_csv(\"cook_county_train.csv\", index_col='Unnamed: 0')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "227d2c41-4391-4611-af7c-b9777fbff92d",
      "cell_type": "markdown",
      "source": "# Exploratory Data Analysis",
      "metadata": {}
    },
    {
      "id": "7a462f50-493c-49bc-ac11-fe672f7619c6",
      "cell_type": "code",
      "source": "# 204792 observations and 62 features in training data\nassert initial_data.shape == (204792, 62)\n# Sale Price is provided in the training data\nassert 'Sale Price' in initial_data.columns.values",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "25fbc139-2d16-450c-8466-ae777533fb60",
      "cell_type": "code",
      "source": "initial_data.columns.values",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c34884de-aec2-4f34-9c9d-bd38a7b09056",
      "cell_type": "code",
      "source": "initial_data['Description'][0]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a65b6f5-6486-46ff-ae7f-4a8d5553deb8",
      "cell_type": "markdown",
      "source": "View of the distribution, a limited range of house prices from [$0, $1,000,000].",
      "metadata": {}
    },
    {
      "id": "23fdf49e-8f11-45de-a568-3c3c606cb8df",
      "cell_type": "code",
      "source": "no_right_outliers = initial_data[initial_data['Sale Price'] <= 1000000]['Sale Price']\nplt.hist(no_right_outliers, bins=200)\nplt.xlabel(\"Sale Price\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Sale Price\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "23614398-0e5b-41ca-92a9-5fdc86119410",
      "cell_type": "markdown",
      "source": "The proportion of how many buildings sold for over \\\\$1,000,000.",
      "metadata": {}
    },
    {
      "id": "62a06e5c-1bb3-44df-b7b1-99ae880fde98",
      "cell_type": "code",
      "source": "len(initial_data[initial_data['Sale Price'] > 1000000]) / len(initial_data[initial_data['Sale Price'] >= 500])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2b57fdd-83ec-41af-917f-2b30514345c8",
      "cell_type": "code",
      "source": "training_data = initial_data[initial_data['Sale Price'] >= 500].copy()\ntraining_data['Log Sale Price'] = np.log(training_data['Sale Price'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5d93c1e9-0e43-4f63-81fe-107a0c40bb95",
      "cell_type": "code",
      "source": "def plot_distribution(data, label):\n    fig, axs = plt.subplots(nrows=2)\n\n    sns.distplot(\n        data[label], \n        ax=axs[0]\n    )\n    sns.boxplot(\n        x=data[label],\n        width=0.3, \n        ax=axs[1],\n        showfliers=False,\n    )\n\n    # Align axes\n    spacer = np.max(data[label]) * 0.05\n    xmin = np.min(data[label]) - spacer\n    xmax = np.max(data[label]) + spacer\n    axs[0].set_xlim((xmin, xmax))\n    axs[1].set_xlim((xmin, xmax))\n\n    # Remove some axis text\n    axs[0].xaxis.set_visible(False)\n    axs[0].yaxis.set_visible(False)\n    axs[1].yaxis.set_visible(False)\n\n    # Put the two plots together\n    plt.subplots_adjust(hspace=0)\n    fig.suptitle(\"Distribution of \" + label)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ee3a7a52-116f-4f4d-968c-50a81d8b8c0f",
      "cell_type": "code",
      "source": "plot_distribution(training_data, label='Log Sale Price');",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cef25b3b-cd8a-41ef-b77d-d65b5c41b93e",
      "cell_type": "code",
      "source": "def remove_outliers(data, variable, lower=-np.inf, upper=np.inf):\n    filtered_data = data[(data[variable] > lower) & (data[variable] < upper)].copy()\n    return filtered_data",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9dc3c353-3729-4c6e-81e3-92fc0e1dbd97",
      "cell_type": "markdown",
      "source": "Check for duplicates",
      "metadata": {}
    },
    {
      "id": "71fc8085-66af-494e-bf83-d0c501e36d13",
      "cell_type": "code",
      "source": "tr_val_data = pd.read_csv(\"cook_county_train_val.csv\", index_col='Unnamed: 0')\ntest_data = pd.read_csv(\"cook_county_contest_test.csv\", index_col='Unnamed: 0')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "112c87ad-5dde-4434-a33a-43e1a4466438",
      "cell_type": "code",
      "source": "duplicates = tr_val_data[tr_val_data.duplicated(keep=False)]\nduplicate_counts = duplicates.groupby(duplicates.columns.tolist()).size()\ncount_duplicate_properties = (duplicate_counts > 1).sum()\n\ncount_duplicate_rows_to_remove = duplicate_counts.sum() - count_duplicate_properties\n\nprint(\"There are \", count_duplicate_properties, \"unique property sales with exact duplicates.\")\nprint(\"There are \", count_duplicate_rows_to_remove, \"a total of duplicate rows that we'll need to remove when we write our cleaning function below.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8d7c2f76-0e6e-46b4-b502-a175c1832e14",
      "cell_type": "code",
      "source": "missing_values = tr_val_data['Sale Price'].isnull().sum() \nmissing_values += tr_val_data['Sale Price'].isna().sum()\nnegative_or_zero_values = (tr_val_data['Sale Price'] <= 0).sum()\nprint(missing_values + negative_or_zero_values)\n\nQ1 = tr_val_data['Sale Price'].quantile(0.25)\nQ3 = tr_val_data['Sale Price'].quantile(0.75)\nIQR = Q3 - Q1\n\nupper_limit = Q3 + 1.5 * IQR\nlarge_outliers = (tr_val_data['Sale Price'] >= upper_limit).sum()\nprint(large_outliers)\n\nlower_limit = Q1 + 1.5 * IQR\nsmall_outliers = (tr_val_data['Sale Price'] <= lower_limit).sum()\nprint(small_outliers)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "79cab4ec-b7b0-495a-a26c-fb99ab2c5bc5",
      "cell_type": "code",
      "source": "pure_market_sales = tr_val_data[tr_val_data['Pure Market Filter'] == 1]\nmax_Sale_Price_filtered = pure_market_sales['Sale Price'].max()\n\nmin_Sale_Price_filtered = pure_market_sales['Sale Price'].min()\n\nprint(\"When considering only pure market sales, the max Sale Price of properties in the data is $\", max_Sale_Price_filtered)\nprint(\"and the min Sale Price is $\", min_Sale_Price_filtered)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e1db4a76-4085-4fac-98f4-4fd240a17d21",
      "cell_type": "code",
      "source": "\ndef clean_data(data):\n    da = data.copy()\n    da = da.drop_duplicates()\n    da = da[da['Pure Market Filter'] == 1]    \n    return da",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "235dc2be-8aea-4c36-baf7-87d2fd1e5b61",
      "cell_type": "markdown",
      "source": "# Feature Engineering",
      "metadata": {}
    },
    {
      "id": "70cfc3e6-171d-4c6d-ae18-6d72c3be5219",
      "cell_type": "code",
      "source": "def add_total_bedrooms(data):\n    with_rooms = data.copy()\n\n    bedrooms_regex = r'(\\d+) of which are bedrooms'\n    \n    with_rooms['Bedrooms'] = with_rooms['Description'].str.extract(bedrooms_regex).astype(float).fillna(0).astype(int)\n    \n    \n    return with_rooms\n\ntraining_data = add_total_bedrooms(training_data)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7d5104f2-b678-403c-b12f-146abb7d5517",
      "cell_type": "markdown",
      "source": "Find if there is an association between Bedrooms and Log Sale Price",
      "metadata": {}
    },
    {
      "id": "9fad2599-6e66-4302-b35b-06379bd4b71e",
      "cell_type": "code",
      "source": "sns.boxplot(x='Bedrooms', y='Log Sale Price', data=training_data)\n\nplt.xlabel('Number of Bedrooms')\nplt.ylabel('Log Sale Price')\nplt.title('Distribution of Log Sale Price by Number of Bedrooms')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a28b4cfb-be8e-446b-96c1-f516b691cbfe",
      "cell_type": "markdown",
      "source": "Now looking at the relationship between neighborhood and sale prices of the houses",
      "metadata": {}
    },
    {
      "id": "0df50055-5255-4c3f-acbb-dc0f13bdc7e2",
      "cell_type": "code",
      "source": "num_neighborhoods = training_data['Neighborhood Code'].nunique()\nnum_neighborhoods",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5753e7d5-d9bf-4600-8725-1ac5eee7deb8",
      "cell_type": "code",
      "source": "neighborhood_sales_count = training_data['Neighborhood Code'].value_counts()\ntop_20 = neighborhood_sales_count.head(20).index\n\nin_top_20_neighborhoods = training_data[training_data['Neighborhood Code'].isin(top_20)].copy()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "668062b2-9353-41dd-a232-fb86cfa3794e",
      "cell_type": "code",
      "source": "def plot_categorical(neighborhoods):\n    fig, axs = plt.subplots(nrows=2)\n\n    sns.boxplot(\n        x='Neighborhood Code',\n        y='Log Sale Price',\n        data=neighborhoods,\n        ax=axs[0],\n    )\n\n    sns.countplot(\n        x='Neighborhood Code',\n        data=neighborhoods,\n        ax=axs[1],\n    )\n\n    # Draw median price\n    axs[0].axhline(\n        y=training_data['Log Sale Price'].median(), \n        color='red',\n        linestyle='dotted'\n    )\n\n    # Label the bars with counts\n    for patch in axs[1].patches:\n        x = patch.get_bbox().get_points()[:, 0]\n        y = patch.get_bbox().get_points()[1, 1]\n        axs[1].annotate(f'{int(y)}', (x.mean(), y), ha='center', va='bottom')\n\n    # Format x-axes\n    axs[1].set_xticklabels(axs[1].xaxis.get_majorticklabels(), rotation=90)\n    axs[0].xaxis.set_visible(False)\n\n    # Narrow the gap between the plots\n    plt.subplots_adjust(hspace=0.01)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "16b6d472-9aad-4af8-bc6a-27aa43ae1250",
      "cell_type": "code",
      "source": "plot_categorical(neighborhoods=in_top_20_neighborhoods)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0a929df8-f2ba-4efe-a723-2f4ee5bf2e05",
      "cell_type": "code",
      "source": "def find_expensive_neighborhoods(data, n=3, metric=np.median):\n    \"\"\"\n    Output:\n      a list of the the neighborhood codes of the top n highest-priced neighborhoods as measured by the metric function\n    \"\"\"\n    neighborhoods = data.groupby('Neighborhood Code')['Log Sale Price'].apply(metric)\n    neighborhoods = neighborhoods.sort_values(ascending=False).head(n).index\n    \n    return [int(code) for code in neighborhoods]\n    # return list(neighborhoods.index)\n    \nexpensive_neighborhoods = find_expensive_neighborhoods(training_data, 3, np.median)\nexpensive_neighborhoods",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "222afcd0-368d-4fd6-bb56-1aa5aa5156c1",
      "cell_type": "code",
      "source": "def add_in_expensive_neighborhood(data, expensive_neighborhoods):\n\n    data['in_expensive_neighborhood'] = data['Neighborhood Code'].isin(expensive_neighborhoods).astype(int)\n    return data\n\nexpensive_neighborhoods = find_expensive_neighborhoods(training_data, 3, np.median)\ntraining_data = add_in_expensive_neighborhood(training_data, expensive_neighborhoods)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2578b441-5843-4a3a-a297-a1fa6d318db7",
      "cell_type": "markdown",
      "source": "Looking into Roof Material",
      "metadata": {}
    },
    {
      "id": "98b9be69-c2e4-4b4e-b4b4-d9fa62fc7ad0",
      "cell_type": "code",
      "source": "def substitute_roof_material(data):\n    roof_material_mapping = {\n        1: 'Shingle/Asphalt',\n        2: 'Tar&Gravel',\n        3: 'Slate',\n        4: 'Shake', \n        5: 'Tile',\n        6: 'Other'\n    }\n    \n    data_mapped = data.copy()\n    data_mapped['Roof Material'] = data_mapped['Roof Material'].replace(roof_material_mapping)\n    return data_mapped\n    \ntraining_data_mapped = substitute_roof_material(training_data)\ntraining_data_mapped.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2c8648f2-7ce3-4ad0-8d41-7c9b7aa9c0e8",
      "cell_type": "code",
      "source": "from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ec33b095-e901-4bb8-8f07-64e3d60e734b",
      "cell_type": "code",
      "source": "def ohe_roof_material(data):\n    ohe = OneHotEncoder()\n    ohe.fit(data[['Roof Material']])\n    ohe.get_feature_names_out()\n    ohe.transform(data[['Roof Material']])\n    \n    roof_material_encoded = ohe.transform(data[['Roof Material']]).todense()\n    \n    roof_material_df = pd.DataFrame(roof_material_encoded, columns=ohe.get_feature_names_out(), index=data.index)\n    \n    data_with_ohe = data.copy()\n    data_with_ohe = data_with_ohe.join(roof_material_df)\n    \n    return data_with_ohe\ntraining_data_ohe = ohe_roof_material(training_data_mapped)\ntraining_data_ohe.filter(regex='^Roof Material').head(5)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aa2174c5-fd08-4946-bf29-514a85171527",
      "cell_type": "markdown",
      "source": "# Cross Validation",
      "metadata": {}
    },
    {
      "id": "9529ff77-34bc-44b8-9002-493e43e6ffa8",
      "cell_type": "code",
      "source": "def train_val_split(data):\n    \"\"\" \n    Takes in a DataFrame `data` and splits it into two smaller DataFrames \n    named `validation` and `train` where validation is the first 20% of the rows and train \n    is the last 80% of the rows, respectively. \n    \"\"\"\n    da = data.copy()\n    \n    cutoff = int(len(da) * 0.2)\n    \n    validation = da[:cutoff]\n    train = da[cutoff:]\n    \n    return train, validation\n\n\n# Randomize the validation and training sets\ntr_val_data_shuffled = tr_val_data.sample(frac=1, random_state=18)\n\n# Clean the shuffled data\ntr_val_clean = clean_data(tr_val_data_shuffled) \n\n# Create the train/val split on the cleaned, shuffled data:\ntr, val = train_val_split(tr_val_clean)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a7cf09d0-ad38-43bd-8a85-4d746926ed80",
      "cell_type": "markdown",
      "source": "# Fitting a Simple Linear Regression Model",
      "metadata": {}
    },
    {
      "id": "bfe37995-2ca4-4672-8c11-6b2f7f225b3c",
      "cell_type": "code",
      "source": "\n\ndef process_data_m1(df):\n    \"\"\" \n    Takes in a DataFrame of cleaned data and performs feature engineering to use for Model 1.\n\n    Outputs a DataFrame with only the features and response/output used in model 1 (that is `Log Sale Price` , `Log Building Square Feet`)\n \n    \"\"\"\n    \n    data=df.copy()\n    \n    # Add a column \"Log Sale Price\" to the `data` DataFrame:\n\n    data['Log Sale Price'] = np.log(data['Sale Price'])\n    \n    # Add a column \"Log Building Square Feet\" to the `data` DataFrame:\n\n    data['Log Building Square Feet'] = np.log(data['Building Square Feet'])\n    \n    # Select the feature and the output/response used in model 1:\n    \n    data = data[['Log Building Square Feet', 'Log Sale Price']]\n    \n    return data\n\n\n\n# Process both the training and validation data: \nprocessed_train_m1 = process_data_m1(tr)\n\nprocessed_val_m1 = process_data_m1(val)\n\n\n# Create X (dataframe) and Y (series) to use to train the model:\nX_train_m1 = processed_train_m1.drop(columns = \"Log Sale Price\")\ny_train_m1 = processed_train_m1[\"Log Sale Price\"]\n\n\n# Create X (dataframe) and Y (series) to use to validate the model:\nX_valid_m1 = processed_val_m1.drop(columns = \"Log Sale Price\")\ny_valid_m1 = processed_val_m1[\"Log Sale Price\"]\n\n# Take a look at the results\nprint(\"Training Data: X\")\ndisplay(X_train_m1.head())\nprint(\"Training Data: y\")\ndisplay(y_train_m1.head())\n\n\nprint(\"Validation Data: X\")\ndisplay(X_valid_m1.head())\nprint(\"Validation Data: y\")\ndisplay(y_valid_m1.head())\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7749e8ae-97d8-41a3-a8d7-999bd21244d1",
      "cell_type": "code",
      "source": "linear_model_m1 = lm.LinearRegression()\n\n# Fit the model using the processed training data:\n\nlinear_model_m1.fit(X_train_m1, y_train_m1)\n\n# Compute the predicted y values from linear model 1 (in units log sale price) \n# using the training data as input:\n\ny_predict_train_m1 = linear_model_m1.predict(X_train_m1)\n\n# Compute the predicted y values from linear model 1 (in units log(sale price))\n# using the validation data as input:\n\ny_predict_valid_m1 = linear_model_m1.predict(X_valid_m1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "cffae4d8-d983-4f4f-97df-3ef4c8db4a80",
      "cell_type": "code",
      "source": "def rmse(predicted, actual):\n    # Calculates RMSE from actual and predicted values\n    \n    mean_squared_diff = np.mean((actual - predicted) ** 2)\n    return np.sqrt(mean_squared_diff)\n    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "319e28f3-da68-488e-9ba4-dc1d8e2cbd54",
      "cell_type": "code",
      "source": "model_names=[\"M1: log(bsqft)\", \"M2\", \"M3\"]\n\n# Create arrays where we can keep track of training and validation RMSE for each model\n\ntraining_error_log = np.zeros(4)\nvalidation_error_log = np.zeros(4)\n\ntraining_error = np.zeros(4)\nvalidation_error = np.zeros(4)\n\n# Array to track cross validation errors average RMSE errors  \n\ncv_error = np.zeros(4)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "506ab846-9634-41a6-b57c-5254e6fe7eff",
      "cell_type": "code",
      "source": "# Training and validation RMSE for the model (in units log sale price)\n\ntraining_error_log[0] = rmse(y_train_m1, y_predict_train_m1)\nvalidation_error_log[0]= rmse(y_valid_m1, y_predict_valid_m1)\n\n\n# Training and validation RMSE for the model (in its original dollar values before the log transform)\n\ntraining_error[0] = rmse(np.exp(y_train_m1), np.exp(y_predict_train_m1))\nvalidation_error[0] = rmse(np.exp(y_valid_m1), np.exp(y_predict_valid_m1))\n\nprint(\"1st Model \\nTraining RMSE: $ {}\\nValidation RMSE: $ {}\\n\".format(training_error[0], validation_error[0]))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9ae6f913-c094-44e5-9b4e-375f05036d43",
      "cell_type": "code",
      "source": "# Cross Validation\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5) \n\ni = 1\n\na = []\n\nfor train_idx, valid_idx in kf.split(tr_val_clean):\n    print (\"positional (iloc) indices for training data for fold\", i)\n    print (train_idx)\n    print (\"positional (iloc) indices for validation data for fold\", i)\n    print (valid_idx)\n    i = i+1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4fac9362-f939-457e-a621-e70f8c4bf3c2",
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold\nfrom sklearn.base import clone\n\ndef cross_validate_rmse(model, X, y):\n    '''\n    Split the X and y data into 5 subsets.\n    For each subset, \n        - Fit a model holding out that subset.\n        - Compute the RMSE (in units dollars, not log(dollars) on that subset (the validation set).\n    Return the average RMSE of these 5 folds.\n    '''\n    # Make a copy of the model to use in this function\n    model = clone(model)\n\n    # Initialize sklearn's KFold object \n    kf = KFold(n_splits=5)  \n\n    # Create a list to store the validation_rmse for each fold\n    validation_rmse = []\n    \n    for train_idx, valid_idx in kf.split(X):\n\n        split_X_train, split_X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        split_Y_train, split_Y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n        # Fit the model on the training split:\n        model.fit(split_X_train, split_Y_train)\n        \n        # Compute the RMSE (in units dollars, not log(dollars)) on the validation split:\n        y_predict_valid = model.predict(split_X_valid)\n        error = rmse(np.exp(split_Y_valid), np.exp(y_predict_valid))\n    \n\n        validation_rmse.append(error)\n        \n\n        #Return the average validation rmse across all cross-validation splits.\n\n    cv_error = np.mean(validation_rmse)\n              \n        \n    return cv_error\n       \n    \n# Create a new model to use for cross validation of m1 \nlinear_model_m1_cv = lm.LinearRegression()\n\n\nprocessed_full_m1 = process_data_m1(tr_val_clean)\n\nX_full_m1 = processed_full_m1.drop(columns='Log Sale Price')\ny_full_m1 = processed_full_m1['Log Sale Price']\n\ncv_error_m1  = cross_validate_rmse(linear_model_m1_cv, X_full_m1, y_full_m1)\n\ncv_error[0] = cv_error_m1\n\nprint(\"1st Model Cross Validation RMSE: {}\".format(cv_error[0]))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "965f6f92-ba98-42de-9d05-610f2307f9a6",
      "cell_type": "markdown",
      "source": "# Visualizing RMSE",
      "metadata": {}
    },
    {
      "id": "5087e44f-596f-4ce7-974e-25e9e1a804cf",
      "cell_type": "code",
      "source": "import plotly.graph_objects as go\n\nfig = go.Figure([\ngo.Bar(x = model_names, y = training_error, name=\"Training RMSE\"),\ngo.Bar(x = model_names, y = validation_error, name=\"Validation RMSE\"),\ngo.Bar(x = model_names, y = cv_error, name=\"Cross Val RMSE\")\n])\n\nfig.update_yaxes(range=[180000,260000], title=\"RMSE\")\nfig",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "07c844bc-5234-45fc-92f7-46eb3fd8dd9a",
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n\nresiduals = y_valid_m1 - y_predict_valid_m1\n\nx_plt1 = y_predict_valid_m1\ny_plt1 = residuals\n\nx_plt2 = y_valid_m1\ny_plt2 = residuals\n\n\n\nax[0].scatter(x_plt1, y_plt1, alpha=.25)\nax[0].axhline(0, c='black', linewidth=1)\nax[0].set_xlabel(r'Predicted Log(Sale Price)')\nax[0].set_ylabel(r'Residuals: Log(Sale Price) - Predicted Log(Sale Price)');\nax[0].set_title(\"Model 1 Val Data: Residuals vs. Predicted Log(Sale Price)\")\n\nax[1].scatter(x_plt2, y_plt2, alpha=.25)\nax[1].axhline(0, c='black', linewidth=1)\nax[1].set_xlabel(r'Log(Sale Price)')\nax[1].set_ylabel(r'Residuals: Log(Sale Price) - Predicted Log(Sale Price)');\nax[1].set_title(\"Model 1 Val Data: Residuals vs. Log(Sale Price)\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1cf54314-58b5-460b-862f-1d2a3115dd0a",
      "cell_type": "markdown",
      "source": "# Adding More Predictors ",
      "metadata": {}
    },
    {
      "id": "365b6f94-0b31-4ade-b143-0ebdcc596d92",
      "cell_type": "code",
      "source": "def process_data_candidates(df):\n    \n    data = df.copy()\n    \n    data[\"Log Sale Price\"] = np.log(data[\"Sale Price\"])\n    \n    # Create Log Building Square Feet column\n    data[\"Log Building Square Feet\"] = np.log(data[\"Building Square Feet\"])\n    \n    \n    # Create Bedrooms\n    data = add_total_bedrooms(data)\n     \n   \n    # Update Roof Material feature with names\n    data = substitute_roof_material(data)\n\n    \n    # Select columns for comparing residuals\n    data = data[['Log Building Square Feet',  'Roof Material', 'Bedrooms', 'Log Sale Price']]\n\n    return data\n\n    \nvalid_comp = process_data_candidates(val)\n    \nvalid_comp = valid_comp.assign(M1residuals_log=y_valid_m1 - y_predict_valid_m1)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "984c62ae-5916-41d3-a249-fa2ed736a1f5",
      "cell_type": "code",
      "source": "import plotly.express as px\n\npx.box(valid_comp, x='Bedrooms', y='M1residuals_log')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9786cd92-3729-4bf5-9566-fdaf21dc2289",
      "cell_type": "code",
      "source": "px.box(valid_comp, x='Roof Material', y='M1residuals_log')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "01155d14-06f0-41a0-8f16-31e196e335c0",
      "cell_type": "markdown",
      "source": "# Multiple Linear Regression Model",
      "metadata": {}
    },
    {
      "id": "fb42899d-8d50-4491-a8a7-ea22db52e709",
      "cell_type": "code",
      "source": "#Process the Data\ndef substitute_roof_material(data):\n    replacements = {\n        'Roof Material': {\n            1: 'Shingle/Asphalt',\n            2: 'Tar&Gravel',\n            3: 'Slate',\n            4: 'Shake',\n            5: 'Tile',\n            6: 'Other',\n        }\n    }\n\n    data = data.replace(replacements)\n    return data\n\ndef process_data_m2(df):\n\n    data = df.copy()\n\n    data['Log Sale Price'] = np.log(data['Sale Price'])\n    data['Log Building Square Feet'] = np.log(data['Building Square Feet'])\n    \n    data = substitute_roof_material(data)\n\n    \n    data = data[['Log Sale Price', 'Log Building Square Feet', 'Roof Material']]\n    data = ohe_roof_material(data)\n    data = data.drop(columns=['Roof Material'])\n    \n    \n    return data\n\n    \n# Process the data for Model 2\nprocessed_train_m2 = process_data_m2(tr)\n\nprocessed_val_m2 = process_data_m2(val)\n\n\n# Create X (dataframe) and Y (series) to use in the model\nX_train_m2 = processed_train_m2.drop(columns = 'Log Sale Price')\ny_train_m2 = processed_train_m2['Log Sale Price']\n\nX_valid_m2 = processed_val_m2.drop(columns = 'Log Sale Price')\ny_valid_m2 = processed_val_m2['Log Sale Price']\n\n\n# Take a look at the result\ndisplay(X_train_m2.head())\ndisplay(y_train_m2.head())\n\ndisplay(X_valid_m2.head())\ndisplay(y_valid_m2.head())\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0c966c6-ca43-4c47-a203-4a60d99a1468",
      "cell_type": "code",
      "source": "#Create and Fit a Multiple Linear Regression Model\n\nprocessed_train_m2 = process_data_m2(tr)\nprocessed_val_m2 = process_data_m2(val)\n\nX_train_m2 = processed_train_m2.drop(columns = 'Log Sale Price')\ny_train_m2 = processed_train_m2['Log Sale Price']\n\nX_valid_m2 = processed_val_m2.drop(columns = 'Log Sale Price')\ny_valid_m2 = processed_val_m2['Log Sale Price']\n\nlinear_model_m2 = lm.LinearRegression()\nlinear_model_m2.fit(X_train_m2, y_train_m2)\n\ny_predict_train_m2 = linear_model_m2.predict(X_train_m2)\n\ny_predict_valid_m2 = linear_model_m2.predict(X_valid_m2)\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "17803e52-0b09-49e8-94ae-f9c98a029df1",
      "cell_type": "code",
      "source": "# Evaluate the RMSE for the model\n\nfrom sklearn.metrics import mean_squared_error\ny_predict_train_m2_original = np.exp(y_predict_train_m2)\ny_predict_valid_m2_original = np.exp(y_predict_valid_m2)\n\ntraining_rmse = np.sqrt(mean_squared_error(np.exp(y_train_m2), y_predict_train_m2_original))\nvalidation_rmse = np.sqrt(mean_squared_error(np.exp(y_valid_m2), y_predict_valid_m2_original))\n\n\ntraining_error[1] = training_rmse\nvalidation_error[1] = validation_rmse\n\n\nprint(\"2nd Model \\nTraining RMSE: $ {}\\nValidation RMSE: $ {}\\n\".format(training_error[1], validation_error[1]))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6452d453-994e-44f4-a295-ba24778f27f5",
      "cell_type": "code",
      "source": "# Conduct 5-fold cross validation for model and output CV RMSE\n\n# Create a new model to use for cross validation of m2 \nlinear_model_m2_cv = lm.LinearRegression()\n\n# Process the entire cleaned training_val dataset using the m2 pipeline\nprocessed_full_m2 = process_data_m2(tr_val_clean)\n\n# Split the processed_full_m2 Dataset into X and Y to use in models.\nX_full_m2 = processed_full_m2.drop(columns='Log Sale Price')\ny_full_m2 = processed_full_m2['Log Sale Price']\n\n\n# Run cross_validate_rmse function:\ncv_error_m2  = cross_validate_rmse(linear_model_m2_cv, X_full_m2, y_full_m2)\n\n# Save the cross validation error for model 1 in our list to compare different models:\n\ncv_error[1] = cv_error_m2\n\nprint(\"2nd Model Cross Validation RMSE: {}\".format(cv_error[1]))\n\n\n\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2d7608a-ab19-4d29-afb6-86eb89607b5d",
      "cell_type": "code",
      "source": "# Plot bar graph comparing RMSEs of Model 2 and Model 1 and side-by-side residuals\n\nmodel_names[1] = \"M2: log(bsqft)+Roof\"\n\nfig = go.Figure([\ngo.Bar(x = model_names, y = training_error, name=\"Training RMSE\"),\ngo.Bar(x = model_names, y = validation_error, name=\"Validation RMSE\"),\ngo.Bar(x = model_names, y = cv_error, name=\"Cross Val RMSE\")\n])\n\nfig.update_yaxes(range=[180000,260000], title=\"RMSE\")\n\nfig\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "949843cf-f542-4a5c-9562-b97ba6191299",
      "cell_type": "code",
      "source": "# Plot 2 side-by-side residual plots for validation data\n\nfig, ax = plt.subplots(1,2, figsize=(15, 5))\n\n\nx_plt1 = linear_model_m2.predict(X_valid_m2)\ny_plt1 = y_valid_m2 - linear_model_m2.predict(X_valid_m2)\n\nx_plt2 = y_valid_m2\ny_plt2 = y_valid_m2 - linear_model_m2.predict(X_valid_m2)\n\n\nax[0].scatter(x_plt1, y_plt1, alpha=.25)\nax[0].axhline(0, c='black', linewidth=1)\nax[0].set_xlabel(r'Predicted Log(Sale Price)')\nax[0].set_ylabel(r'Residuals: Log(Sale Price) - Predicted Log(Sale Price)');\nax[0].set_title(\"Model 2 Val Data: Residuals vs. Predicted Log(Sale Price)\")\n\nax[1].scatter(x_plt2, y_plt2, alpha=.25) \nax[1].axhline(0, c='black', linewidth=1)\nax[1].set_xlabel(r'Log(Sale Price)')\nax[1].set_ylabel(r'Residuals: Log(Sale Price) - Predicted Log(Sale Price)');\nax[1].set_title(\"Model 2 Val Data: Residuals vs. Log(Sale Price)\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "77ed7f08-6606-4186-844e-d011988d082c",
      "cell_type": "markdown",
      "source": "# Improving the Model",
      "metadata": {}
    },
    {
      "id": "2ff09ab1-c946-44de-bfb8-3eb00ab626ec",
      "cell_type": "code",
      "source": "# Choosing features to add\nplt.figure(figsize=(12, 8))\nplt.subplot(2, 2, 1)\nsns.scatterplot(x=tr_val_clean['Building Square Feet'], y=tr_val_clean['Sale Price'])\nplt.title(\"Building Square Feet vs Sale Price\")\nplt.xscale('log')\nplt.yscale('log')\n\nplt.subplot(2, 2, 2)\nsns.scatterplot(x=tr_val_clean['Land Square Feet'], y=tr_val_clean['Sale Price'])\nplt.title(\"Land Square Feet vs Sale Price\")\nplt.xscale('log')\nplt.yscale('log')\n\nplt.subplot(2, 2, 3)\nsns.scatterplot(x=tr_val_clean['Age'], y=tr_val_clean['Sale Price'])\nplt.title(\"Age vs Sale Price\")\nplt.xscale('log')\nplt.yscale('log')\n\nplt.subplot(2, 2, 4)\nsns.scatterplot(x=tr_val_clean['Fireplaces'], y=tr_val_clean['Sale Price'])\nplt.title(\"Fireplaces vs Sale Price\")\nplt.xscale('log')\nplt.yscale('log')\n\nplt.tight_layout()\nplt.show()\n# Show work in this cell exploring data to determine which feature to add",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bac7a63a-f5de-4582-8bbc-553bcc6106a2",
      "cell_type": "code",
      "source": "# Process the Data\n\ndef process_data_m3(df):\n    \n    data = df.copy()\n        \n    data['Log Sale Price'] = np.log(data['Sale Price'])\n    data['Log Building Square Feet'] = np.log(data['Building Square Feet'])\n    data['Log Land Square Feet'] = np.log(data['Land Square Feet'])\n    \n    \n    data = substitute_roof_material(data)\n    \n    data = data[['Log Sale Price', 'Roof Material', 'Log Land Square Feet', \n                 'Log Building Square Feet']]\n    data = ohe_roof_material(data) \n\n    data = data.drop(columns=['Roof Material'])\n    \n    \n    return data\n\nprocessed_train_m3 = process_data_m3(tr) \n\nprocessed_val_m3 = process_data_m3(val) \n\n# Create X (Dataframe) and y (series) to use to train the model\nX_train_m3 = processed_train_m3.drop(columns= 'Log Sale Price')\ny_train_m3 = processed_train_m3['Log Sale Price']\n\nX_valid_m3 = processed_val_m3.drop(columns= 'Log Sale Price')\ny_valid_m3 = processed_val_m3['Log Sale Price']\n\n\n# Take a look at the result\ndisplay(X_train_m3.head())\ndisplay(y_train_m3.head())\n\ndisplay(X_valid_m3.head())\ndisplay(y_valid_m3.head())\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "864b8d09-1900-46a8-9930-777a625af8e7",
      "cell_type": "code",
      "source": "# Create and Fit a Multiple Linear Regression Model\nprocessed_train_m3 = process_data_m3(tr)\nprocessed_val_m3 = process_data_m3(val)\n\nX_train_m3 = processed_train_m3.drop(columns = 'Log Sale Price')\ny_train_m3 = processed_train_m3['Log Sale Price']\n\nX_valid_m2 = processed_val_m3.drop(columns = 'Log Sale Price')\ny_valid_m3 = processed_val_m3['Log Sale Price']\n\nlinear_model_m3 = lm.LinearRegression()\nlinear_model_m3.fit(X_train_m3, y_train_m3)\n\ny_predict_train_m3 = linear_model_m3.predict(X_train_m3)\n\ny_predict_valid_m3 = linear_model_m3.predict(X_valid_m3)\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6ee66649-bbb5-4699-b60b-6de813c028a1",
      "cell_type": "code",
      "source": "# Evaluate the RMSE for your model\n\ny_predict_train_m3_original = np.exp(y_predict_train_m3)\ny_predict_valid_m3_original = np.exp(y_predict_valid_m3)\n\ntraining_rmse = np.sqrt(mean_squared_error(np.exp(y_train_m3), y_predict_train_m3_original))\nvalidation_rmse = np.sqrt(mean_squared_error(np.exp(y_valid_m3), y_predict_valid_m3_original))\n\n\ntraining_error[2] = training_rmse\nvalidation_error[2] = validation_rmse\n\n\n(print(\"3rd Model \\nTraining RMSE: $ {}\\nValidation RMSE: {}\\n\"\n       .format(training_error[2], validation_error[2]))\n)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6997d54c-3196-4424-bdfc-6aefce9d7a5f",
      "cell_type": "code",
      "source": "\n# Conduct 5-fold cross validation for model and output RMSE\n\nlinear_model_m3_cv = lm.LinearRegression()\n\nprocessed_full_m3 = process_data_m3(tr_val_clean)\n\n# Split the processed_full_m3 Dataset into X and y to use in models.\nX_full_m3 = processed_full_m3.drop(columns='Log Sale Price')\ny_full_m3 = processed_full_m2['Log Sale Price']\n\n\n# Run cross_validate_rmse function:\ncv_error_m3  = cross_validate_rmse(linear_model_m3_cv, X_full_m3, y_full_m3)\n\n# Save the cross validation error for model 3 in our list to compare different models:\n\ncv_error[2] = cv_error_m3\n\nprint(\"3rd Model Cross Validation RMSE: {}\".format(cv_error[2]))\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b572b7b1-ba97-4795-9c8c-c860d49c2f1a",
      "cell_type": "code",
      "source": "# Plot bar graph all 3 models\n\nmodel_names[2] = \"M3: log(lsqft)+log(bsqft)+Roof\"\n\n\nfig = go.Figure([\ngo.Bar(x = model_names, y = training_error, name=\"Training RMSE\"),\ngo.Bar(x = model_names, y = validation_error, name=\"Validation RMSE\"),\ngo.Bar(x = model_names, y = cv_error, name=\"Cross Val RMSE\")\n])\n\nfig.update_yaxes(range=[180000,260000], title=\"RMSE\")\n\nfig\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4fbc8d02-2f8f-4b58-b919-4a8ed85d9c69",
      "cell_type": "code",
      "source": "# Plot 2 side-by-side residual plots for validation data\n\nfig, ax = plt.subplots(1,2, figsize=(15, 5))\n\n\nx_plt1 = linear_model_m3.predict(X_valid_m3)\ny_plt1 = y_valid_m3 - linear_model_m3.predict(X_valid_m3)\n\nx_plt2 = y_valid_m3\ny_plt2 = y_valid_m3 - linear_model_m3.predict(X_valid_m3)\n\n\nax[0].scatter(x_plt1, y_plt1, alpha=.25)\nax[0].axhline(0, c='black', linewidth=1)\nax[0].set_xlabel(r'Predicted Log(Sale Price)')\nax[0].set_ylabel(r'Residuals: Log(Sale Price) - Predicted Log(Sale Price)');\nax[0].set_title(\"Model 3 Val Data: Residuals vs. Predicted Log(Sale Price)\")\n\nax[1].scatter(x_plt2, y_plt2, alpha=.25)\nax[1].axhline(0, c='black', linewidth=1)\nax[1].set_xlabel(r'Log(Sale Price)')\nax[1].set_ylabel(r'Residuals: Log(Sale Price) - Predicted Log(Sale Price)');\nax[1].set_title(\"Model 3 Val Data: Residuals vs. Log(Sale Price)\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8d2bb163-4f05-498d-a537-aaab6511ed8e",
      "cell_type": "markdown",
      "source": "# Final Improved Model",
      "metadata": {}
    },
    {
      "id": "770431d7-d3b6-4fec-9508-5f157758956f",
      "cell_type": "code",
      "source": "tr['Log Sale Price'] = np.log(tr['Sale Price'])\nval['Log Sale Price'] = np.log(val['Sale Price'])\n# print(tr)\ndef ohe_column(data, column_name):\n    oh_enc = OneHotEncoder() \n    oh_enc.fit(data[[column_name]])\n    \n    dummies = pd.DataFrame(oh_enc.transform(data[[column_name]]).todense(),\n                           columns=oh_enc.get_feature_names_out(),\n                           index=data.index)\n    \n    return data.join(dummies)\n\ndef process_data_ec(df, dataset_type):\n\n    data = df.copy()\n    data['Log Building Square Feet'] = np.log(data['Building Square Feet'])\n    data['Log Land Square Feet'] = np.log(data['Land Square Feet'])\n    data['Sqrt Age'] = np.sqrt(data['Age'])   \n\n    data = substitute_roof_material(data)\n    \n    if dataset_type == 3:\n        data = data[['Log Building Square Feet', 'Log Land Square Feet', 'Sqrt Age', \n            'Roof Material', 'Basement', 'Basement Finish', 'Attic Type', 'Attic Finish',\n            'Garage 1 Size', 'Garage Indicator', 'Construction Quality', \n            'Central Heating', 'Other Heating', 'Central Air', 'Site Desirability', \n            'Fireplaces', 'Floodplain', 'O\\'Hare Noise',  \n            'Road Proximity', 'Property Class', 'Sale Quarter', 'Garage 1 Material',\n            'Garage 1 Attachment', 'Garage 1 Area','Garage 2 Attachment', \n            'Garage 2 Area', 'Design Plan', 'Town Code', 'Cathedral Ceiling']]\n    else:\n        data = data[['Log Sale Price', 'Log Building Square Feet', 'Log Land Square Feet', \n            'Sqrt Age', 'Roof Material', 'Basement', 'Basement Finish', 'Attic Type', \n            'Attic Finish', 'Garage 1 Size', 'Garage Indicator', 'Construction Quality',\n            'Central Heating', 'Other Heating', 'Central Air', 'Site Desirability',\n            'Fireplaces', 'Floodplain', 'O\\'Hare Noise', 'Road Proximity', 'Property Class',\n            'Sale Quarter', 'Garage 1 Material', 'Garage 1 Attachment', 'Garage 1 Area',\n            'Garage 2 Attachment', 'Garage 2 Area', 'Design Plan', 'Town Code',\n            'Cathedral Ceiling']]\n    \n    data = ohe_column(data, 'Basement')\n    data = ohe_column(data, 'Basement Finish')\n    data = ohe_column(data, 'Attic Type')\n    data = ohe_column(data, 'Attic Finish')\n    data = ohe_column(data, 'Garage 1 Size')\n    data = ohe_column(data, 'Garage Indicator')\n    data = ohe_column(data, 'Central Heating')\n    data = ohe_column(data, 'Other Heating')\n    data = ohe_column(data, 'Central Air')\n    data = ohe_column(data, 'Design Plan')\n    data = ohe_column(data, 'Cathedral Ceiling')\n    data = ohe_column(data, 'Construction Quality')\n    data = ohe_column(data, 'Site Desirability')\n    data = ohe_column(data, 'Garage 1 Material')\n    data = ohe_column(data, 'Garage 1 Attachment')\n    data = ohe_column(data, 'Garage 1 Area')\n    data = ohe_column(data, 'Garage 2 Attachment')\n    data = ohe_column(data, 'Garage 2 Area')\n    data = ohe_column(data, 'Fireplaces')\n    data = ohe_column(data, 'Floodplain')\n    data = ohe_column(data, 'O\\'Hare Noise')\n    data = ohe_column(data, 'Road Proximity')\n    data = ohe_column(data, 'Property Class')\n    data = ohe_column(data, 'Sale Quarter')\n    data = ohe_column(data, 'Town Code')\n    \n    data = ohe_roof_material(data) \n\n    \n    data = data.drop(columns=['Roof Material', 'Basement', 'Basement Finish', 'Attic Type', \n            'Attic Finish', 'Garage 1 Size', 'Garage Indicator', 'Construction Quality',\n            'Central Heating', 'Other Heating', 'Central Air', 'Site Desirability',\n             'Fireplaces', 'Floodplain', 'O\\'Hare Noise', 'Road Proximity', 'Property Class', \n            'Sale Quarter', 'Garage 1 Material', 'Garage 1 Attachment', 'Garage 1 Area',\n            'Garage 2 Attachment', 'Garage 2 Area', 'Design Plan', 'Town Code', \n                'Cathedral Ceiling'])\n    \n    return data\n\nprocessed_train_ec = process_data_ec(tr, dataset_type=1)\n\nprocessed_val_ec = process_data_ec(val, dataset_type=2)\n\n\nX_train_ec = processed_train_ec.drop(columns='Log Sale Price')\ny_train_ec = processed_train_ec['Log Sale Price']\n\nX_valid_ec = processed_val_ec.drop(columns='Log Sale Price')\ny_valid_ec = processed_val_ec['Log Sale Price']\n\n\n# Take a look at the result\ndisplay(X_train_ec.head())\ndisplay(y_train_ec.head())\n\ndisplay(X_valid_ec.head())\ndisplay(y_valid_ec.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a3b64210-1605-4246-aa30-735c389a2760",
      "cell_type": "code",
      "source": "# Create a Multiple Linear Regression Model\n\nlinear_model_ec = lm.LinearRegression()\nlinear_model_ec.fit(X_train_ec, y_train_ec)\n\ny_predict_train_ec = linear_model_ec.predict(X_train_ec)\n\ny_predict_valid_ec = linear_model_ec.predict(X_valid_ec)\n\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c35b82c4-8275-4dad-a9ee-e3d13ac2f2d0",
      "cell_type": "code",
      "source": "# Evaluate the RMSE for your model\n\n\n# Training and test errors for the model \n\ny_predict_train_ec_original = np.exp(y_predict_train_ec)\ny_predict_valid_ec_original = np.exp(y_predict_valid_ec)\n\ntraining_error_ec = np.sqrt(mean_squared_error(np.exp(y_train_ec), y_predict_train_ec_original))\nvalidation_error_ec = np.sqrt(mean_squared_error(np.exp(y_valid_ec), y_predict_valid_ec_original))\n\n\n(print(\"Final Modal \\nTraining RMSE:$ {}\\nValidation RMSE:$ {}\\n\"\n       .format(training_error_ec, validation_error_ec))\n)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f206a300-a264-4aac-b775-3ade0f95f993",
      "cell_type": "code",
      "source": "fig = go.Figure([\ngo.Bar(x = [\"Final Model\"], y = [training_error_ec], name=\"Training RMSE\"),\ngo.Bar(x = [\"Final Model\"], y = [validation_error_ec], name=\"Validation RMSE\"),\n\n])\n\n\nfig\nfig.update_yaxes(range=[140000,260000], title=\"RMSE\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3df15df6-ecf5-425b-be3d-32b9d7fdbaaa",
      "cell_type": "code",
      "source": "# Plot 2 side-by-side residual plots for validation data\n\nfig, ax = plt.subplots(1,2, figsize=(15, 5))\n\n\nx_plt1 = y_predict_valid_ec\ny_plt1 = y_valid_ec - y_predict_valid_ec\n\nx_plt2 = y_valid_ec\ny_plt2 = y_valid_ec - y_predict_valid_ec\n\n\nax[0].scatter(x_plt1, y_plt1, alpha=.25)\nax[0].axhline(0, c='black', linewidth=1)\nax[0].set_xlabel(r'Predicted Log(Sale Price)')\nax[0].set_ylabel(r'Residuals: Log(Sale Price) - Predicted Log(Sale Price)');\nax[0].set_title(\"EC Val Data: Residuals vs. Predicted Log(Sale Price)\")\n\nax[1].scatter(x_plt2, y_plt2, alpha=.25)\nax[1].axhline(0, c='black', linewidth=1)\nax[1].set_xlabel(r'Log(Sale Price)')\nax[1].set_ylabel(r'Residuals: Log(Sale Price) - Predicted Log(Sale Price)');\nax[1].set_title(\"EC Val Data: Residuals vs. Log(Sale Price)\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}